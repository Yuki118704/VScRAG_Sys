"""
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã§ãã¾ã™

ä½¿ã„æ–¹:
  py add_to_db.py ãƒ•ã‚¡ã‚¤ãƒ«å.md
  py add_to_db.py ãƒ•ã‚©ãƒ«ãƒ€å
"""

import sys
import os
from pathlib import Path
from RAG_Sys.vector_db import VectorDatabase
from langchain_text_splitters import RecursiveCharacterTextSplitter


# ========== ãƒãƒ£ãƒ³ã‚¯åŒ–è¨­å®š ==========
CHUNK_SIZE = 1000           # 1ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°
CHUNK_OVERLAP = 200         # ãƒãƒ£ãƒ³ã‚¯é–“ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ–‡å­—æ•°
ENABLE_CHUNKING = True      # ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆFalse = è¦‹å‡ºã—ã®ã¿ã§åˆ†å‰²ï¼‰
# ====================================


def read_text_file(file_path: Path) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
        return None


def split_large_chunks(text: str, source: str, max_size: int = CHUNK_SIZE) -> list[tuple[str, str]]:
    """
    å¤§ããªãƒ†ã‚­ã‚¹ãƒˆã‚’é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰²
    
    Args:
        text: åˆ†å‰²ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        source: ã‚½ãƒ¼ã‚¹æƒ…å ±
        max_size: æœ€å¤§ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        
    Returns:
        (ãƒ†ã‚­ã‚¹ãƒˆ, ã‚½ãƒ¼ã‚¹) ã®ãƒªã‚¹ãƒˆ
    """
    if not ENABLE_CHUNKING or len(text) <= max_size:
        return [(text, source)]
    
    # LangChainã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã‚’ä½¿ç”¨
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
    )
    
    chunks = splitter.split_text(text)
    
    # ãƒãƒ£ãƒ³ã‚¯æ•°ãŒ1ã¤ãªã‚‰ç•ªå·ã‚’ä»˜ã‘ãªã„
    if len(chunks) == 1:
        return [(chunks[0], source)]
    
    # è¤‡æ•°ãƒãƒ£ãƒ³ã‚¯ã®å ´åˆã¯ç•ªå·ã‚’ä»˜ã‘ã‚‹
    result = []
    for i, chunk in enumerate(chunks, 1):
        chunk_source = f"{source} (part {i}/{len(chunks)})"
        result.append((chunk, chunk_source))
    
    return result


def split_markdown_by_sections(content: str, source_file: str) -> list[tuple[str, str]]:
    """Markdownã‚’è¦‹å‡ºã—ã”ã¨ã«åˆ†å‰²"""
    sections = []
    current_section = []
    current_title = "å°å…¥"
    
    for line in content.split('\n'):
        if line.startswith('## '):
            # å‰ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
            if current_section:
                section_text = '\n'.join(current_section).strip()
                if section_text:
                    sections.append((section_text, f"{source_file} - {current_title}"))
            
            # æ–°ã—ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
            current_title = line[3:].strip()
            current_section = [line]
        else:
            current_section.append(line)
    
    # æœ€å¾Œã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿å­˜
    if current_section:
        section_text = '\n'.join(current_section).strip()
        if section_text:
            sections.append((section_text, f"{source_file} - {current_title}"))
    
    return sections


def process_file(file_path: Path) -> list[tuple[str, str]]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚½ãƒ¼ã‚¹ã®ãƒšã‚¢ã‚’è¿”ã™"""
    print(f"  ğŸ“„ {file_path.name}")
    
    content = read_text_file(file_path)
    if not content:
        return []
    
    # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«åˆ†å‰²
    if file_path.suffix.lower() in ['.md', '.markdown']:
        sections = split_markdown_by_sections(content, file_path.name)
        
        # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã•ã‚‰ã«ãƒãƒ£ãƒ³ã‚¯åŒ–
        all_chunks = []
        for text, source in sections:
            chunks = split_large_chunks(text, source)
            all_chunks.extend(chunks)
        
        total_chunks = len(all_chunks)
        section_count = len(sections)
        
        if total_chunks > section_count:
            print(f"     â†’ {section_count}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ {total_chunks}ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
        else:
            print(f"     â†’ {section_count}ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²")
        
        return all_chunks
    else:
        # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦ä¿å­˜
        chunks = split_large_chunks(content, str(file_path.name))
        if len(chunks) > 1:
            print(f"     â†’ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
        return chunks


def main():
    if len(sys.argv) < 2:
        print("ä½¿ã„æ–¹: python add_to_db.py <ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€>")
        print("ä¾‹:")
        print("  python add_to_db.py test_story.md")
        print("  python add_to_db.py ./documents")
        sys.exit(1)
    
    target_path = Path(sys.argv[1])
    
    if not target_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {target_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸ“š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ")
    print("=" * 60)
    print(f"ãƒãƒ£ãƒ³ã‚¯åŒ–è¨­å®š: {'æœ‰åŠ¹' if ENABLE_CHUNKING else 'ç„¡åŠ¹'}")
    if ENABLE_CHUNKING:
        print(f"  - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {CHUNK_SIZE}æ–‡å­—")
        print(f"  - ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {CHUNK_OVERLAP}æ–‡å­—")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
    files_to_process = []
    
    if target_path.is_file():
        files_to_process.append(target_path)
    elif target_path.is_dir():
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ‹¡å¼µå­
        extensions = ['.txt', '.md', '.markdown', '.rst']
        for ext in extensions:
            files_to_process.extend(target_path.glob(f'**/*{ext}'))
    
    if not files_to_process:
        print("âŒ å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    print(f"\nğŸ“‚ {len(files_to_process)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™\n")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    all_texts = []
    all_sources = []
    
    for file_path in files_to_process:
        sections = process_file(file_path)
        for text, source in sections:
            all_texts.append(text)
            all_sources.append(source)
    
    print(f"\nâœ“ åˆè¨ˆ {len(all_texts)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™å®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ 
    print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ä¸­...")
    
    # MCPã‚µãƒ¼ãƒãƒ¼ã¨åŒã˜å ´æ‰€ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ
    db_path = Path(__file__).parent / "RAG_Sys" / "mcp_faiss_db"
    
    db = VectorDatabase(
        collection_name="copilot_rag",
        persist_directory=str(db_path)
    )
    
    print(f"ğŸ’¾ {len(all_texts)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ä¸­...")
    
    # ãƒãƒƒãƒå‡¦ç†ï¼ˆ10ä»¶ãšã¤ï¼‰
    batch_size = 10
    for i in range(0, len(all_texts), batch_size):
        batch_texts = all_texts[i:i+batch_size]
        batch_sources = all_sources[i:i+batch_size]
        
        metadatas = [{"source": src} for src in batch_sources]
        db.add_texts(batch_texts, metadatas=metadatas)
        
        processed = min(i + batch_size, len(all_texts))
        print(f"  é€²æ—: {processed}/{len(all_texts)} ä»¶å®Œäº†")
    
    print("\n" + "=" * 60)
    print("âœ… å®Œäº†ï¼")
    print(f"   è¿½åŠ ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(all_texts)} ä»¶")
    print("=" * 60)


if __name__ == "__main__":
    main()
